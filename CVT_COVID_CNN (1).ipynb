{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CVT COVID CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-8ytuvJcgj6",
        "colab_type": "text"
      },
      "source": [
        "**Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMb6_EtmUqQC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9286aede-dc2d-4bea-c7ae-241b7c858185"
      },
      "source": [
        "import os, shutil, PIL, keras\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from pathlib import Path\n",
        "from keras import layers, models, optimizers, regularizers\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.applications.densenet import DenseNet121\n",
        "from keras.applications.nasnet import NASNetMobile\n",
        "from keras.models import Model, Input, Sequential,load_model\n",
        "from keras.layers import AveragePooling2D, Dense, Dropout\n",
        "from keras.callbacks import Callback, ModelCheckpoint\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4WrKERGci9f",
        "colab_type": "text"
      },
      "source": [
        "**Mounting Drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpV-Zec6yLLQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "29b8d094-a8d1-429b-905a-3dc2b2434305"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CX5wlMumcoPI",
        "colab_type": "text"
      },
      "source": [
        "**Getting paths and directories, setting global variables**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kLNYTW8Uvvk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_batch_size = 10\n",
        "val_batch_size = 10\n",
        "num_epochs = 20\n",
        "image_size = (224, 224)\n",
        "\n",
        "data_dir = '/content/drive/My Drive/CVT'\n",
        "\n",
        "train_dir = os.path.join(data_dir, 'train')\n",
        "Path(train_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "validation_dir = os.path.join(data_dir, 'validation')\n",
        "Path(validation_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "test_dir = os.path.join(data_dir, 'test')\n",
        "Path(test_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "train_txt = os.path.join(data_dir, 'TrainingTXT')\n",
        "Path(train_txt).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "validation_txt = os.path.join(data_dir, 'ValidationTXT')\n",
        "Path(validation_txt).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "test_txt = os.path.join(data_dir, 'TestingTXT')\n",
        "Path(test_txt).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "augmented_dir = os.path.join(data_dir, 'augmented')\n",
        "Path(augmented_dir).mkdir(parents=True, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q27W8uEcc0t7",
        "colab_type": "text"
      },
      "source": [
        "**Reading Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYM6P9fIcear",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read(datadir, fname):\n",
        "    with open(os.path.join(datadir, fname), 'r') as f:\n",
        "        data_fnames = [line.rstrip('\\n') for line in f.readlines()]\n",
        "    return data_fnames\n",
        "\n",
        "def createData(dirname, arr, original_data_dir):\n",
        "    Path(dirname).mkdir(parents=True, exist_ok=True)\n",
        "    for fname in arr:\n",
        "        src = os.path.join(data_dir, original_data_dir, fname)\n",
        "        dst = os.path.join(dirname, fname)\n",
        "        shutil.copyfile(src, dst)\n",
        "\n",
        "def initData():\n",
        "    training_covid = read(train_txt, 'trainCT_COVID.txt')\n",
        "    #createData(os.path.join(train_dir, 'COVID'), training_covid, 'CT_COVID')\n",
        "    training_non_covid = read(train_txt, 'trainCT_NonCOVID.txt')\n",
        "    #createData(os.path.join(train_dir, 'NonCOVID'), training_non_covid, 'CT_NonCOVID')\n",
        "    num_train_samples = len(training_covid) + len(training_non_covid)\n",
        "\n",
        "    validation_covid = read(validation_txt, 'valCT_COVID.txt')\n",
        "    #createData(os.path.join(validation_dir, 'COVID'), validation_covid, 'CT_COVID')\n",
        "    validation_non_covid = read(validation_txt, 'valCT_NonCOVID.txt')\n",
        "    #createData(os.path.join(validation_dir, 'NonCOVID'), validation_non_covid, 'CT_NonCOVID')\n",
        "    num_val_samples = len(validation_covid) + len(validation_non_covid)\n",
        "\n",
        "    test_covid = read(test_txt, 'testCT_COVID.txt')\n",
        "    #createData(os.path.join(test_dir, 'COVID'), test_covid, 'CT_COVID')\n",
        "    test_non_covid = read(test_txt, 'testCT_NonCOVID.txt')\n",
        "    #createData(os.path.join(test_dir, 'NonCOVID'), test_non_covid, 'CT_NonCOVID')\n",
        "\n",
        "    return (num_train_samples, num_val_samples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2LNqtq4dB_U",
        "colab_type": "text"
      },
      "source": [
        "**Preprocessing and Data Augmentation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llKDPX0XaYQv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add_noise_contrast(img):\n",
        "    VARIABILITY = 50\n",
        "    deviation = VARIABILITY*random.random()\n",
        "    noise = np.random.normal(0, deviation, img.shape)\n",
        "    img += noise\n",
        "    np.clip(img, 0., 255.)\n",
        "    return img\n",
        "\n",
        "def preprocess_data():\n",
        "    train_datagen = ImageDataGenerator(\n",
        "                rescale=1./255,\n",
        "                rotation_range=40,\n",
        "                width_shift_range=0.2,\n",
        "                height_shift_range=0.2,\n",
        "                brightness_range=[0.4, 1.0],\n",
        "                # shear_range=0.2,\n",
        "                zoom_range=0.4,\n",
        "                horizontal_flip=True,\n",
        "                vertical_flip=True,\n",
        "                fill_mode=\"nearest\",\n",
        "                preprocessing_function=add_noise_contrast\n",
        "                )\n",
        "\n",
        "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "\n",
        "    test_generator = test_datagen.flow_from_directory(\n",
        "        test_dir,\n",
        "        target_size=image_size,\n",
        "        class_mode='binary'\n",
        "    )\n",
        "\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=image_size,\n",
        "        batch_size=train_batch_size,\n",
        "        class_mode='binary'\n",
        "    )\n",
        "\n",
        "    validation_generator = train_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=image_size,\n",
        "        batch_size=val_batch_size,\n",
        "        class_mode='binary'\n",
        "    )\n",
        "\n",
        "    return (train_generator, validation_generator, test_generator)\n",
        "\n",
        "def test_augmentation():\n",
        "    datagen = ImageDataGenerator(\n",
        "            rescale=1./255,\n",
        "            rotation_range=20,\n",
        "            width_shift_range=0.2,\n",
        "            height_shift_range=0.2,\n",
        "            brightness_range=[0.4, 1.0],\n",
        "            # shear_range=0.2,\n",
        "            zoom_range=0.4,\n",
        "            horizontal_flip=True,\n",
        "            vertical_flip=True,\n",
        "            fill_mode=\"nearest\",\n",
        "            preprocessing_function=add_noise_contrast\n",
        "            )\n",
        "\n",
        "    fnames_covid = [os.path.join(train_dir, 'COVID', fname) for fname in os.listdir(os.path.join(train_dir, 'COVID'))]\n",
        "    fnames_noncovid = [os.path.join(train_dir, 'NonCOVID', fname) for fname in os.listdir(os.path.join(train_dir, 'NonCOVID'))]\n",
        "    fnames = fnames_covid + fnames_noncovid\n",
        "    print(len(fnames))\n",
        "    img_path = fnames[1]\n",
        "    print(img_path)\n",
        "\n",
        "    img = image.load_img(img_path, target_size=image_size)\n",
        "    x = image.img_to_array(img)\n",
        "    x = x.reshape((1,) + x.shape)\n",
        "    print(x.shape)\n",
        "    i = 0\n",
        "    for batch in datagen.flow(x, batch_size=1):\n",
        "        plt.figure(i)\n",
        "        imgplot = plt.imshow(image.array_to_img(batch[0]))\n",
        "        i += 1\n",
        "        if (i % 10 == 0):\n",
        "            break\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdBbbHPKcSGB",
        "colab_type": "text"
      },
      "source": [
        "**Function to calculate f1 metric**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8i0YDSC4gO3e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#to be used to calculate f1 metric\n",
        "def get_f1(y_true, y_pred): #taken from old keras source code\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
        "    return f1_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMsyPQAwcHS8",
        "colab_type": "text"
      },
      "source": [
        "**Model build without transfer learning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWZSRy2oeulw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#original model build without transfer learning\n",
        "def build():\n",
        "    model = models.Sequential()\n",
        "    # (150, 150) means the input image has to be 150 x 150 px\n",
        "    model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
        "                            input_shape=(image_size[0], image_size[1], 3)))\n",
        "    model.add(layers.MaxPooling2D(2, 2))\n",
        "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(layers.MaxPooling2D(2, 2))\n",
        "    model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "    model.add(layers.MaxPooling2D(2, 2))\n",
        "    model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "    model.add(layers.MaxPooling2D(2, 2))\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dropout(0.5))\n",
        "    model.add(layers.Dense(512, activation='relu'))\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer=optimizers.RMSprop(lr=1e-4),\n",
        "                  metrics=['acc'])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgjrZEqUbixv",
        "colab_type": "text"
      },
      "source": [
        "**Model build using Densenet121 architecture.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fuhLgnbezaa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_transfer():\n",
        "  IMG_SHAPE = (224, 224, 3)\n",
        "\n",
        "  pre_trained_model = DenseNet121(input_shape=IMG_SHAPE, include_top=False)\n",
        "\n",
        "  regularizer = tf.keras.regularizers.l1_l2(l1=0.01, l2=0.01)\n",
        "\n",
        "  for layer in pre_trained_model.layers:\n",
        "      for attr in ['kernel_regularizer']:\n",
        "          if hasattr(layer, attr):\n",
        "            setattr(layer, attr, regularizer)\n",
        "\n",
        "  last_layer = pre_trained_model.get_layer('relu')\n",
        "  last_output = last_layer.output\n",
        "\n",
        "\n",
        "  x = layers.Flatten()(last_output)\n",
        "  x = layers.Dense(512, activation='relu')(x)\n",
        "  x = layers.Dropout(0.5)(x)     \n",
        "  # x = layers.SpatialDropout2D(0.5)(x)                     \n",
        "  x = layers.Dense (1, activation='sigmoid')(x)           \n",
        "\n",
        "  model = Model( pre_trained_model.input, x) \n",
        "\n",
        "  model.compile(loss='binary_crossentropy',\n",
        "              optimizer=optimizers.Adam(lr=1e-4),\n",
        "              metrics=['acc', get_f1])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vu_zyx_ofcmF",
        "colab_type": "text"
      },
      "source": [
        "**Model build using NASNetMobile architecture.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3g2SdsETfqN8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_transfer2():\n",
        "  pre_trained_NASNetMobile_model = NASNetMobile(input_shape= (224,224,3), include_top = False, weights = 'imagenet')\n",
        "\n",
        "  pre_trained_NASNetMobile_model.trainable = True\n",
        "\n",
        "  regularizer = tf.keras.regularizers.l1_l2(l1=0.01, l2=0.01)\n",
        "\n",
        "  for layer in pre_trained_NASNetMobile_model.layers:\n",
        "      for attr in ['kernel_regularizer']:\n",
        "          if hasattr(layer, attr):\n",
        "            setattr(layer, attr, regularizer)\n",
        "\n",
        "  X = layers.Flatten()(pre_trained_NASNetMobile_model.output)\n",
        "  X = layers.Dense(512, activation='relu')(X)\n",
        "  X = layers.Dropout(0.5)(X)\n",
        "  X = layers.Dense(1, activation = 'sigmoid')(X)\n",
        "\n",
        "  model = Model(pre_trained_NASNetMobile_model.input, X)  \n",
        "\n",
        "  model.compile(loss = 'binary_crossentropy',\n",
        "                optimizer = optimizers.Adam(learning_rate=0.0001),\n",
        "                metrics = ['acc', get_f1, keras.metrics.BinaryAccuracy()])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWS6YMszbzVu",
        "colab_type": "text"
      },
      "source": [
        "**Function to graph model metrics (acc, loss, f1)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xelv1QnajFN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def graph_metrics(history):\n",
        "    acc = history.history['acc']\n",
        "    val_acc = history.history['val_acc']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    f1 = history.history['get_f1']\n",
        "    val_f1 = history.history['val_get_f1']\n",
        "\n",
        "    epochs = range(1, len(acc) + 1)\n",
        "\n",
        "    plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.figure()\n",
        "\n",
        "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.figure()\n",
        "\n",
        "    plt.plot(epochs, f1, 'bo', label='Training f1')\n",
        "    plt.plot(epochs, val_f1, 'b', label='Validation f1')\n",
        "    plt.title('Training and validation f1')\n",
        "    plt.legend\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0DPy1ZUb_ZP",
        "colab_type": "text"
      },
      "source": [
        "**Train and Save Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68pXsIEYtr6O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save(model):\n",
        "    # serialize model to JSON\n",
        "    model_json = model.to_json()\n",
        "    with open(\"model.json\", \"w\") as json_file:\n",
        "        json_file.write(model_json)\n",
        "    # serialize weights to HDF5\n",
        "    model.save_weights(\"model_no_overfit.h5\")\n",
        "    print(\"Saved model to disk\")\n",
        "\n",
        "def train(model, train_generator, validation_generator, num_train_samples, num_val_samples):\n",
        "    history = model.fit(\n",
        "                    train_generator,\n",
        "                    steps_per_epoch=num_train_samples // train_batch_size,\n",
        "                    epochs=num_epochs,\n",
        "                    validation_data=validation_generator,\n",
        "                    validation_steps=num_val_samples // val_batch_size\n",
        "            )\n",
        "    return history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQz6K_Wvems6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model = build()\n",
        "# model = build_transfer2()\n",
        "model = build_transfer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_CE0e-UeoLv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "a7a7c317-0f3a-4f7b-b3b9-c22856050232"
      },
      "source": [
        "num_train_samples, num_val_samples = initData()\n",
        "train_generator, validation_generator, test_generator = preprocess_data()\n",
        "history = train(model, train_generator, validation_generator, num_train_samples, num_val_samples)\n",
        "graph_metrics(history)\n",
        "save(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 203 images belonging to 2 classes.\n",
            "Found 425 images belonging to 2 classes.\n",
            "Found 118 images belonging to 2 classes.\n",
            "Epoch 1/20\n",
            " 3/42 [=>............................] - ETA: 14:44 - loss: 2.1717 - acc: 0.4667 - get_f1: 0.4665"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-5043caab45d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_train_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_val_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_train_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_val_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mgraph_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-5171c494f7ce>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_generator, validation_generator, num_train_samples, num_val_samples)\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_val_samples\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mval_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             )\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m                 \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m                 \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m                 initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m         \u001b[0;31m# Case 2: Symbolic tensors or Numpy array-like.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    218\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                                             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                                             reset_metrics=False)\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3790\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3791\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3792\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3794\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m     \"\"\"\n\u001b[0;32m-> 1605\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1643\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1644\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1645\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bmu6NxOUcCgQ",
        "colab_type": "text"
      },
      "source": [
        "**Evaluate Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EkPtNJ5Tu0j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_loss, test_acc, test_f1, x = model.evaluate(test_generator, verbose=1)\n",
        "print(test_loss)\n",
        "print(test_acc)\n",
        "print(test_f1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-S9C6sG8LkZC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "9d028dc8-59cb-4b66-d27f-ec6567495af8"
      },
      "source": [
        "test_augmentation()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-29f6d65fb265>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_augmentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-69d1ac43f8d0>\u001b[0m in \u001b[0;36mtest_augmentation\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m             )\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mfnames_covid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'COVID'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'COVID'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0mfnames_noncovid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NonCOVID'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NonCOVID'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mfnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfnames_covid\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfnames_noncovid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_dir' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjYGYiPNdL1V",
        "colab_type": "text"
      },
      "source": [
        "**Currently unsuccesful model builds using ResNet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IwpH6oTdKYI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#unsuccesful builds with resnet transfer learning\n",
        "#\n",
        "#\n",
        "# def build_transfer2():\n",
        "#   baseModel = ResNet50(weights=\"imagenet\", include_top=False,\n",
        "# \tinput_tensor=Input(shape=(224, 224, 3)))\n",
        "#   # construct the head of the model that will be placed on top of the\n",
        "#   # the base model\n",
        "#   headModel = baseModel.output\n",
        "#   headModel = layers.AveragePooling2D(pool_size=(7, 7))(headModel)\n",
        "#   headModel = layers.Flatten(name=\"flatten\")(headModel)\n",
        "#   headModel = layers.Dense(256, activation=\"relu\")(headModel)\n",
        "#   headModel = layers.Dropout(0.5)(headModel)\n",
        "#   headModel = layers.Dense(1, activation=\"softmax\")(headModel)\n",
        "#   # place the head FC model on top of the base model (this will become\n",
        "#   # the actual model we will train)\n",
        "#   model = Model(inputs=baseModel.input, outputs=headModel)\n",
        "#   # loop over all layers in the base model and freeze them so they will\n",
        "#   # *not* be updated during the training process\n",
        "#   for layer in baseModel.layers:\n",
        "# \t  layer.trainable = False\n",
        "#   #opt = optimizers.Adam(lr=1e-4, decay=1e-4 / 50)\n",
        "#   #opt = optimizer=optimizers.RMSprop(lr=1e-4)\n",
        "#   model.compile(loss=\"binary_crossentropy\", optimizer='adam',\n",
        "# \tmetrics=[\"accuracy\", get_f1])\n",
        "#   return model\n",
        "# def build_transfer():\n",
        "#   input_tuple = (224, 224, 3)\n",
        "#   resnet = ResNet50(include_top=False, weights='imagenet', input_shape= input_tuple)\n",
        "#   #resnet.summary()\n",
        "#   # for layer in resnet.layers:\n",
        "#   # layer.trainable = False\n",
        "#   model = models.Sequential()\n",
        "#   model.add(resnet)\n",
        "#   model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
        "#                              # input_shape=(image_size[0], image_size[1], 3)))\n",
        "#   model.add(layers.MaxPooling2D(2, 2, padding='same'))\n",
        "#   model.add(layers.Dropout(0.5))\n",
        "#     # model.summary()\n",
        "#   model.add(layers.Conv2D(16, (3, 3), activation='relu', padding='same'))\n",
        "#   model.add(layers.MaxPooling2D(2, 2, padding='same'))\n",
        "#     # model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "#     # model.add(layers.MaxPooling2D(2, 2, padding='same'))\n",
        "#     # model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "#     # model.add(layers.MaxPooling2D(2, 2, padding='same'))\n",
        "#   model.add(layers.Flatten())\n",
        "#   model.add(layers.Dropout(0.5))\n",
        "#   model.add(layers.Dense(512, activation='relu'))\n",
        "#   model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "#   opt = optimizers.Adam(lr=1e-4, decay=1e-4 / 50)\n",
        "#   # opt = optimizer=optimizers.RMSprop(lr=1e-4)\n",
        "#   model.compile(loss='binary_crossentropy',\n",
        "#                   optimizer=opt,\n",
        "#                   metrics=[get_f1, 'acc'])\n",
        "#   return model"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}